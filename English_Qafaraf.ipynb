{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mametecho/Machine-Translation/blob/main/English_Qafaraf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN2-KVHIgV_e"
      },
      "source": [
        "Import Necessary library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gegn8f4Ch0qT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras import layers, models\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMJHSPTvfk5U"
      },
      "source": [
        "Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "QHVrHxrXff7y",
        "outputId": "05032cab-445e-426d-b32a-d26cbf84e640"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        english_sent  \\\n",
              "0  I heard the rusting  of palm leaves carried by...   \n",
              "1                                         I felt sad   \n",
              "2    patriots sacrifice themselves for their country   \n",
              "3                  Books are exported in paper sacks   \n",
              "4  When I heard a dress rusting, I knew my mother...   \n",
              "5                   The man in the accident was safe   \n",
              "6                                 I rue coming here    \n",
              "7                    Rude children are badly bahaved   \n",
              "8                     footballers have to obey rules   \n",
              "9               the president rules his country well   \n",
              "\n",
              "                                          afar_sent  \n",
              "0             Dananal yeerren qungah farracto oobbe  \n",
              "1                                    Anu nadaamiteh  \n",
              "2        Agat kacannoytit isinni baaxoh fida yakken  \n",
              "3                Buukat baaxok baaxo kafaqat ruuban  \n",
              "4  Sari hargufto oobbe waqdi yiina temeetem eexegeh  \n",
              "5             Qawwalayla edde tekke num nagay raaqe  \n",
              "6                  Anu akke abe mamaatat nadaamitah  \n",
              "7              adbisinna le urri gexgexit umaane le  \n",
              "8   koqsoh digira mari le madqah amrisimam faxximta  \n",
              "9             ummuunoh abba isi baaxo nagay xiinisa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8004cf96-87ae-48ab-8fc8-fc7aaae7add1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sent</th>\n",
              "      <th>afar_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I heard the rusting  of palm leaves carried by...</td>\n",
              "      <td>Dananal yeerren qungah farracto oobbe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I felt sad</td>\n",
              "      <td>Anu nadaamiteh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>patriots sacrifice themselves for their country</td>\n",
              "      <td>Agat kacannoytit isinni baaxoh fida yakken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Books are exported in paper sacks</td>\n",
              "      <td>Buukat baaxok baaxo kafaqat ruuban</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When I heard a dress rusting, I knew my mother...</td>\n",
              "      <td>Sari hargufto oobbe waqdi yiina temeetem eexegeh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The man in the accident was safe</td>\n",
              "      <td>Qawwalayla edde tekke num nagay raaqe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I rue coming here</td>\n",
              "      <td>Anu akke abe mamaatat nadaamitah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Rude children are badly bahaved</td>\n",
              "      <td>adbisinna le urri gexgexit umaane le</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>footballers have to obey rules</td>\n",
              "      <td>koqsoh digira mari le madqah amrisimam faxximta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>the president rules his country well</td>\n",
              "      <td>ummuunoh abba isi baaxo nagay xiinisa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8004cf96-87ae-48ab-8fc8-fc7aaae7add1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8004cf96-87ae-48ab-8fc8-fc7aaae7add1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8004cf96-87ae-48ab-8fc8-fc7aaae7add1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-da40b635-41bc-4dc6-a1cc-3fe7b3e7f800\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da40b635-41bc-4dc6-a1cc-3fe7b3e7f800')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-da40b635-41bc-4dc6-a1cc-3fe7b3e7f800 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 32451,\n  \"fields\": [\n    {\n      \"column\": \"english_sent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32428,\n        \"samples\": [\n          \"I value my health, but I also indulge in unhealthy habits. \",\n          \"i have no fever\",\n          \"i will try\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"afar_sent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30180,\n        \"samples\": [\n          \"Usug isi nafsi niqbah\",\n          \"Xayi udduurih teknoloojih dadal baad elle gorrisna gita mamilaaginna\",\n          \"Barad ankel yaniih\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "usecols = ['English', 'Afar']\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/English_afaraf NMT/Dataset/English_afarafih.csv\",encoding='latin1', delimiter=',',usecols=usecols)\n",
        "df.dropna(axis = 0, inplace = True)\n",
        "df.rename(columns = {\"English\" : \"english_sent\", \"Afar\" : \"afar_sent\"}, inplace = True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ZBTUmmf4uk",
        "outputId": "6a75aa71-d7f1-483d-8512-49683c0aca62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32451, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apz0_CdxgOH8"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RV8FqvFLfEm9"
      },
      "outputs": [],
      "source": [
        "contractions = {\n",
        "        \"i'm\": \"i am\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"'ll\": \" will\",\n",
        "        \"'ve\": \" have\",\n",
        "        \"'re\": \" are\",\n",
        "        \"'d\": \" would\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"n't\": \" not\",\n",
        "        \"n'\": \"ng\",\n",
        "        \"'bout\": \"about\"\n",
        "    }\n",
        "def expand_contractions(text):\n",
        "        for contraction, expanded in contractions.items():\n",
        "            text = re.sub(r\"\\b{}\\b\".format(contraction), expanded, text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ldt5Drnbi5L9"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(df):\n",
        "    # Lowercase the characters\n",
        "    df[\"english_sent\"] = df[\"english_sent\"].apply(lambda x : x.lower())\n",
        "    df[\"afar_sent\"] = df[\"afar_sent\"].apply(lambda x : x.lower())\n",
        "\n",
        "\n",
        "    # Removing digits\n",
        "    remove_digits = str.maketrans(\"\", \"\",string.digits)\n",
        "    df[\"english_sent\"] = df[\"english_sent\"].apply(lambda x : x.translate(remove_digits))\n",
        "    df[\"afar_sent\"] = df[\"afar_sent\"].apply(lambda x : x.translate(remove_digits))\n",
        "\n",
        "\n",
        "    # Remove special characters\n",
        "    special = set(string.punctuation)\n",
        "    df['english_sent'] = df['english_sent'].apply(lambda x : ''.join(ch for ch in x if ch not in special))\n",
        "    df['afar_sent'] = df['afar_sent'].apply(lambda x : ''.join(ch for ch in x if ch not in special))\n",
        "\n",
        "    # Remove quotes\n",
        "    df['english_sent'] = df['english_sent'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "    df['afar_sent'] = df['afar_sent'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "    # Remove extra spaces\n",
        "    df['english_sent'] = df['english_sent'].apply(lambda x : x.strip())\n",
        "    df['afar_sent'] = df['afar_sent'].apply(lambda x : x.strip())\n",
        "    df['english_sent'] = df['english_sent'].apply(lambda x : re.sub(\" +\",\" \",x))\n",
        "    df['afar_sent'] = df['afar_sent'].apply(lambda x : re.sub(\" +\",\" \",x))\n",
        "\n",
        "\n",
        "    # Add [start] and [end] tags\n",
        "    df[\"afar_sent\"] = df[\"afar_sent\"].apply(lambda x : \"[start] \" + x + \" [end]\")\n",
        "    # Remove contracdiction\n",
        "    df['english_sent'] = df['english_sent'].apply(expand_contractions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dvD2Eipagm4p"
      },
      "outputs": [],
      "source": [
        "# Preprocess text\n",
        "preprocess_text(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x9u0f1xMjpl8"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    afar_vocab = af_vectorization.get_vocabulary()\n",
        "    afar_index_lookup = dict(zip(range(len(afar_vocab)), afar_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = af_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = afar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence[8:-5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W-Ask1AOj87W"
      },
      "outputs": [],
      "source": [
        "# For creating Dataset\n",
        "def format_dataset(eng, af):\n",
        "    eng = eng_vectorization(eng)\n",
        "    af = af_vectorization(af)\n",
        "    return ({\"encoder_inputs\" : eng, \"decoder_inputs\" : af[:, :-1],}, af[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(df):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df[\"english_sent\"].values, df[\"afar_sent\"].values))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2RykyDyimAP8"
      },
      "outputs": [],
      "source": [
        "# Drop rows with Null values\n",
        "df.drop(df[df[\"english_sent\"] == \" \"].index, inplace = True)\n",
        "df.drop(df[df[\"afar_sent\"] == \"[start]  [end]\"].index, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uTNWDHM6mT1G"
      },
      "outputs": [],
      "source": [
        " #Find Sentence Length\n",
        "df[\"eng_sent_length\"] = df[\"english_sent\"].apply(lambda x : len(x.split(' ')))\n",
        "df[\"afar_sent_length\"] = df[\"afar_sent\"].apply(lambda x : len(x.split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J9oMKWt5maZA"
      },
      "outputs": [],
      "source": [
        "# Get sentences with specific length 20\n",
        "df = df[df[\"eng_sent_length\"] <= 20]\n",
        "df = df[df[\"afar_sent_length\"] <= 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UaiCfbfAmgV9"
      },
      "outputs": [],
      "source": [
        "# Take a sample no larger than the DataFrame\n",
        "sample_size = min(300000, len(df))\n",
        "df = df.sample(n=sample_size, random_state=2048)\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LkIhz534mz-E"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting val_data into test_data and val_data\n",
        "test_data, val_data = train_test_split(val_data, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnMklkk0EI-1",
        "outputId": "56a7d09c-378b-45ba-9df6-fba3c0aa21c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25901, 4), (3238, 4), (3238, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_data.shape,  val_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo91GH4Si-UP"
      },
      "source": [
        "Define **Parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2JYYjxski900"
      },
      "outputs": [],
      "source": [
        "# Tranining Hyperparameters\n",
        "batch_size = 64\n",
        "\n",
        "# Model Hyperparameters\n",
        "embed_dim = 128\n",
        "num_heads = 10\n",
        "latent_dim = 2048\n",
        "vocab_size = 20000\n",
        "sequence_length = 20\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAjMPLw6nQV4"
      },
      "source": [
        "Tokenizing Sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XYrKhNZHnOz-"
      },
      "outputs": [],
      "source": [
        "# Using TextVectorization to create sentence vectors\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g-HzCgBdLpeC"
      },
      "outputs": [],
      "source": [
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "af_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization\n",
        ")\n",
        "\n",
        "eng_vectorization.adapt(df[\"english_sent\"].values)\n",
        "af_vectorization.adapt(df[\"afar_sent\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_Wedc4Nbnn3Q"
      },
      "outputs": [],
      "source": [
        "# Savng parameters and weights of both vectorizer\n",
        "pickle.dump({'config': eng_vectorization.get_config(),\n",
        "             'weights': eng_vectorization.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/English_afaraf NMT/eng_vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "pickle.dump({'config': af_vectorization.get_config(),\n",
        "             'weights': af_vectorization.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/English_afaraf NMT/afar_vectorizer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu6Dva0JoOlK"
      },
      "source": [
        "Creating **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "USQTiTXWRV3n"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train_data)\n",
        "val_ds = make_dataset(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjjstOWLuS4"
      },
      "source": [
        "Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zG5HedSApH7R"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embedding = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embedding(inputs)\n",
        "        embedded_positions = self.position_embedding(positions)\n",
        "        return embedded_tokens + embedded_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tPhwfwyopV4Y"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, dropout, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.layer_norm1 = layers.LayerNormalization()\n",
        "        self.layer_norm2 = layers.LayerNormalization()\n",
        "        self.layer_ffn = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        padding_mask = None\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        ffn_input = self.layer_norm1(inputs + attention_output)\n",
        "        ffn_output = self.layer_ffn(ffn_input)\n",
        "        return self.layer_norm2(ffn_input + ffn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7xYwKTeOperG"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, dropout, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.attention1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.layer_ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layer_norm1 = layers.LayerNormalization()\n",
        "        self.layer_norm2 = layers.LayerNormalization()\n",
        "        self.layer_norm3 = layers.LayerNormalization()\n",
        "\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        # Initialize padding_mask\n",
        "        padding_mask = None\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = causal_mask\n",
        "\n",
        "        attention_output1 = self.attention1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out1 = self.layer_norm1(inputs + attention_output1)\n",
        "\n",
        "        attention_output2 = self.attention2(\n",
        "            query=out1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask\n",
        "        )\n",
        "        out2 = self.layer_norm2(out1 + attention_output2)\n",
        "\n",
        "        ffn_output = self.layer_ffn(out2)\n",
        "        return self.layer_norm3(out2 + ffn_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l-723194ucsn"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads, dropout, name=\"encoder_1\")(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads, dropout, name=\"decoder_1\")(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEa747zZ6gnW",
        "outputId": "f582b964-90ad-4bcd-ce08-f9c7465c7eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 128)            2562560   ['encoder_inputs[0][0]']      \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_1 (TransformerEnco  (None, None, 128)            1186304   ['positional_embedding[0][0]']\n",
            " der)                                                                                             \n",
            "                                                                                                  \n",
            " model_1 (Functional)        (None, None, 20000)          6988448   ['decoder_inputs[0][0]',      \n",
            "                                                                     'encoder_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10737312 (40.96 MB)\n",
            "Trainable params: 10737312 (40.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qDoWJ1Tn2b5U"
      },
      "outputs": [],
      "source": [
        "# Compile and train the model\n",
        "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsCs6V5e2e7F",
        "outputId": "7819a8f5-1151-48c2-b766-642749816f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "405/405 [==============================] - 901s 2s/step - loss: 1.9861 - accuracy: 0.7934 - val_loss: 1.3595 - val_accuracy: 0.8235 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "405/405 [==============================] - 882s 2s/step - loss: 1.2500 - accuracy: 0.8325 - val_loss: 1.1507 - val_accuracy: 0.8472 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "405/405 [==============================] - 908s 2s/step - loss: 1.0216 - accuracy: 0.8536 - val_loss: 1.0379 - val_accuracy: 0.8590 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "405/405 [==============================] - 912s 2s/step - loss: 0.8434 - accuracy: 0.8699 - val_loss: 0.9748 - val_accuracy: 0.8653 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "405/405 [==============================] - 861s 2s/step - loss: 0.6941 - accuracy: 0.8847 - val_loss: 0.9250 - val_accuracy: 0.8729 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "248/405 [=================>............] - ETA: 5:16 - loss: 0.5911 - accuracy: 0.8964"
          ]
        }
      ],
      "source": [
        "# Training model\n",
        "history = transformer.fit(train_ds, epochs = 10, validation_data = val_ds, callbacks = [early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLyrX0p2eGQK",
        "outputId": "0db0a6c2-a7e1-48eb-bac2-e37431a41b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score is: 0.19627091742719796\n"
          ]
        }
      ],
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    afar_vocab = af_vectorization.get_vocabulary()\n",
        "    afar_index_lookup = dict(zip(range(len(afar_vocab)), afar_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = af_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = afar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence[8:-5]\n",
        "\n",
        "eng = test_data[\"english_sent\"].values\n",
        "original = test_data[\"afar_sent\"].values\n",
        "translated = [decode_sequence(sent) for sent in eng]\n",
        "bleu = 0\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "for i in range(test_data.shape[0]):\n",
        "    bleu += sentence_bleu([original[i].split()], translated[i].split(), weights=(0.5, 0.5), smoothing_function=smoothie)\n",
        "\n",
        "print(\"BLEU score is:\", bleu / test_data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QxMyUEZJOAxx"
      },
      "outputs": [],
      "source": [
        "# Back-translation\n",
        "def back_translate(df, model, eng_vectorization, af_vectorization):\n",
        "    back_translations = []\n",
        "    for i in range(len(df)):\n",
        "        english_sentence = df.iloc[i][\"english_sent\"]\n",
        "        afar_sentence = decode_sequence(english_sentence, model, eng_vectorization, af_vectorization)\n",
        "        back_translations.append((english_sentence, afar_sentence))\n",
        "\n",
        "    back_translated_df = pd.DataFrame(back_translations, columns=[\"english_sent\", \"afar_sent\"])\n",
        "    return back_translated_df\n",
        "\n",
        "def decode_sequence(input_sentence, model, eng_vectorization, af_vectorization):\n",
        "    afar_vocab = af_vectorization.get_vocabulary()\n",
        "    afar_index_lookup = dict(zip(range(len(afar_vocab)), afar_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = af_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = afar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence[8:-5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ni2gr_2QOHLI"
      },
      "outputs": [],
      "source": [
        "# Create a back-translated dataset\n",
        "back_translated_df = back_translate(df, transformer, eng_vectorization, af_vectorization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iODUnehdOR72",
        "outputId": "44149fa9-4260-4448-9ba6-6c36502eef84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5f39d253cdb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Combine original and back-translated datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_translated_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Combine original and back-translated datasets\n",
        "combined_df = pd.concat([df, back_translated_df])\n",
        "train_data, val_data = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
        "test_data, val_data = train_test_split(val_data, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3T9LGeFOsjX",
        "outputId": "c3ac0fc1-4d4c-47cb-f8b3-cefde8336a7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((51803, 4), (6476, 4), (6475, 4))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape, val_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92sdSxVKHFRg"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train_data)\n",
        "val_ds = make_dataset(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsqI0tGjOfGa",
        "outputId": "34e62d76-62c9-45bf-a73e-77693d3686cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "810/810 [==============================] - 1679s 2s/step - loss: 0.5073 - accuracy: 0.9140 - val_loss: 0.6736 - val_accuracy: 0.9008 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "810/810 [==============================] - 1628s 2s/step - loss: 0.3782 - accuracy: 0.9292 - val_loss: 0.5697 - val_accuracy: 0.9097 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "810/810 [==============================] - 1619s 2s/step - loss: 0.3027 - accuracy: 0.9377 - val_loss: 0.4913 - val_accuracy: 0.9185 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "810/810 [==============================] - 1655s 2s/step - loss: 0.2538 - accuracy: 0.9439 - val_loss: 0.4369 - val_accuracy: 0.9277 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "810/810 [==============================] - 1625s 2s/step - loss: 0.2219 - accuracy: 0.9486 - val_loss: 0.4091 - val_accuracy: 0.9342 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "810/810 [==============================] - 1619s 2s/step - loss: 0.1987 - accuracy: 0.9527 - val_loss: 0.3967 - val_accuracy: 0.9379 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "810/810 [==============================] - 1628s 2s/step - loss: 0.1844 - accuracy: 0.9555 - val_loss: 0.3849 - val_accuracy: 0.9411 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "810/810 [==============================] - 1633s 2s/step - loss: 0.1715 - accuracy: 0.9580 - val_loss: 0.3797 - val_accuracy: 0.9430 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "810/810 [==============================] - 1628s 2s/step - loss: 0.1609 - accuracy: 0.9602 - val_loss: 0.3822 - val_accuracy: 0.9438 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "810/810 [==============================] - 1624s 2s/step - loss: 0.1514 - accuracy: 0.9619 - val_loss: 0.3735 - val_accuracy: 0.9470 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = transformer.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWYdmwXfqIFd"
      },
      "outputs": [],
      "source": [
        "# Saving weights of model\n",
        "transformer.save_weights(\"/content/drive/MyDrive/English_afaraf NMT/eng-af.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAp0b4OQqOAT"
      },
      "source": [
        "Testing Model & Calculating BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dX3fhH9qWg1",
        "outputId": "5cdbb0ce-5b6e-4b13-b343-49cdcab76263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score is: 0.4476192838629354\n"
          ]
        }
      ],
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    afar_vocab = af_vectorization.get_vocabulary()\n",
        "    afar_index_lookup = dict(zip(range(len(afar_vocab)), afar_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = af_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = afar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence[8:-5]\n",
        "\n",
        "eng = test_data[\"english_sent\"].values\n",
        "original = test_data[\"afar_sent\"].values\n",
        "translated = [decode_sequence(sent) for sent in eng]\n",
        "bleu = 0\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "for i in range(test_data.shape[0]):\n",
        "    bleu += sentence_bleu([original[i].split()], translated[i].split(), weights=(0.5, 0.5), smoothing_function=smoothie)\n",
        "\n",
        "print(\"BLEU score is:\", bleu / test_data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "N1PXPvbXRfKA",
        "outputId": "62f8f585-8962-4497-b158-3b1bf90b16ac"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'decode_sequence' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b8b2760e70b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"she is married while i am a bachelor.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"English: {input_sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Translated to Afar: {translated_sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decode_sequence' is not defined"
          ]
        }
      ],
      "source": [
        "input_sentence = \"she is married while i am a bachelor.\"\n",
        "translated_sentence = decode_sequence(input_sentence)\n",
        "print(f\"English: {input_sentence}\")\n",
        "print(f\"Translated to Afar: {translated_sentence}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xTjN0BGgdeILT41_XglKD3RuT7-MPD3E",
      "authorship_tag": "ABX9TyOWGQTPKWcx4Itsa6UHwMHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}